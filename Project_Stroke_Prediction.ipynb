{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/f-assiamah/Projects/blob/main/Project_Stroke_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Import"
      ],
      "metadata": {
        "id": "MasfLyvBUtJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing data\n",
        "import pandas as pd\n",
        "\n",
        "# Specify the path to your CSV file\n",
        "file_path = 'https://drive.google.com/uc?export=download&id=1IKUzSjd0qkuLdyT2iC5O8zbpXb-mK_fT'\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "Vx86JofnUsTy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55eb72fd-81cc-4c02-aaf7-f5cf43528445"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      id  gender   age  hypertension  heart_disease ever_married  \\\n",
            "0   9046    Male  67.0             0              1          Yes   \n",
            "1  51676  Female  61.0             0              0          Yes   \n",
            "2  31112    Male  80.0             0              1          Yes   \n",
            "3  60182  Female  49.0             0              0          Yes   \n",
            "4   1665  Female  79.0             1              0          Yes   \n",
            "\n",
            "       work_type Residence_type  avg_glucose_level   bmi   smoking_status  \\\n",
            "0        Private          Urban             228.69  36.6  formerly smoked   \n",
            "1  Self-employed          Rural             202.21   NaN     never smoked   \n",
            "2        Private          Rural             105.92  32.5     never smoked   \n",
            "3        Private          Urban             171.23  34.4           smokes   \n",
            "4  Self-employed          Rural             174.12  24.0     never smoked   \n",
            "\n",
            "   stroke  \n",
            "0       1  \n",
            "1       1  \n",
            "2       1  \n",
            "3       1  \n",
            "4       1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cleaning and Preprocessing the Dataset"
      ],
      "metadata": {
        "id": "jmoBqP3yUrGq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jmminvGyUiip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3371ada0-d42f-45cb-de25-1a9bdde4f170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-a3ab155dce68>:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['bmi'].fillna(df['bmi'].mean(), inplace=True)\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Some values within the bmi section of this dataset are missing therefore we are replacing the missing values by calculating the mean of the non-missing values.\n",
        "df['bmi'].fillna(df['bmi'].mean(), inplace=True)\n",
        "\n",
        "# Categorical variables cannot be used directly in machine learning models. One-hot encoding converts categorical variables into numerical columns with binary values (0 or 1). This is necessary for machine learning models to process categorical data. We set drop_first=True to avoid unnecessary columns.\n",
        "df = pd.get_dummies(df, columns=['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status'], drop_first=True)\n",
        "\n",
        "# Scale numerical features ensures all numerical data is on a similar scale to ensure no one feature overpowers others.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "df[['age', 'avg_glucose_level', 'bmi']] = scaler.fit_transform(df[['age', 'avg_glucose_level', 'bmi']])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Splitting the Data into Features (x) and Target (y)"
      ],
      "metadata": {
        "id": "6Ajx8HTcVmFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into features (X) and target variable (y). Dropping columns id and stroke as these are irrelevant when training the models to predict if an indidivual has or hasn't had a stroke.\n",
        "X = df.drop(columns=['id', 'stroke'])\n",
        "y = df['stroke']"
      ],
      "metadata": {
        "id": "znWNm9omWP5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Splitting Data into Training and Test Sets"
      ],
      "metadata": {
        "id": "NG6XWR2lWzbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#80% of the data will be used to train the model and 20% of it to test the model.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "YpnnjkL0XBwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training and Evaluating the Models using Test Data with Object Oriented Programming"
      ],
      "metadata": {
        "id": "nCHvOga-X1em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing libraries from scikit-learn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Using OOP to create a 'model' class.\n",
        "class Model:\n",
        "    def __init__(self, model, name):\n",
        "        self.model = model\n",
        "        self.name = name\n",
        "\n",
        "#Training the data with the x and y variables\n",
        "    def train(self, X_train, y_train):\n",
        "        self.model.fit(X_train, y_train)\n",
        "\n",
        "#Evaluating the model performance using 20% of the dataset that was previously split. This will help assess how well the models makes predictions using only the test data.\n",
        "#y_pred is a variable that holds the predictions made by the model.\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        y_pred = self.model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "        return accuracy, conf_matrix\n",
        "\n",
        "#Displaying the results.\n",
        "    def display_results(self, accuracy, conf_matrix):\n",
        "        print(f\"Model: {self.name}\")\n",
        "        print(f\"Accuracy: {accuracy:.2f}\")\n",
        "        print(f\"Confusion Matrix:\\n{conf_matrix}\\n\")\n"
      ],
      "metadata": {
        "id": "Mg3pHVhaXG_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialising the models"
      ],
      "metadata": {
        "id": "TI07J5Fif0HI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise models, setting parameters and preparing the models for training and evaluation.\n",
        "# By using OOP to create a model class, logistic regression, random forest and KNN models are initliased as instances of this class. Thus, allowing for an organised and consistant way to train and evaluate each model.\n",
        "logistic_regression_model = Model(LogisticRegression(random_state=42), \"Logistic Regression\")\n",
        "random_forest_model = Model(RandomForestClassifier(random_state=42), \"Random Forest\")\n",
        "knn_model = Model(KNeighborsClassifier(), \"K-Nearest Neighbors\")\n",
        "decision_tree_model = Model(DecisionTreeClassifier(random_state=42), \"Decision Tree\")\n",
        "\n",
        "#Logistic Regression\n",
        "logistic_regression_model.train(X_train, y_train)\n",
        "accuracy_lr, conf_matrix_lr = logistic_regression_model.evaluate(X_test, y_test)\n",
        "logistic_regression_model.display_results(accuracy_lr, conf_matrix_lr)\n",
        "\n",
        "#Random Forest\n",
        "random_forest_model.train(X_train, y_train)\n",
        "accuracy_rf, conf_matrix_rf = random_forest_model.evaluate(X_test, y_test)\n",
        "random_forest_model.display_results(accuracy_rf, conf_matrix_rf)\n",
        "\n",
        "#KNN\n",
        "knn_model.train(X_train, y_train)\n",
        "accuracy_knn, conf_matrix_knn = knn_model.evaluate(X_test, y_test)\n",
        "knn_model.display_results(accuracy_knn, conf_matrix_knn)\n",
        "\n",
        "#Decision Tree\n",
        "decision_tree_model.train(X_train, y_train)\n",
        "accuracy_dt, conf_matrix_dt = decision_tree_model.evaluate(X_test, y_test)\n",
        "decision_tree_model.display_results(accuracy_dt, conf_matrix_dt)"
      ],
      "metadata": {
        "id": "pAsK5pQ5eheo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic Regression"
      ],
      "metadata": {
        "id": "SrQ-ppu1ZMDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The Logistic Regression model achieved an accuracy of 94%. As a result, 94% of the test cases were predicted correctly.\n",
        "\n",
        "The confusion matrix showed that the model correctly identified 960 people without a stroke, without any mistakes. However, it missed all of the stroke cases and misclassified 62 people who had a stroke as not having one.\n",
        "\n",
        "As a result, while the model is good at predicting non-stroke cases (which makes up the majority of the cases), it struggles to correctly identify actual stroke cases, (which make up the minority)."
      ],
      "metadata": {
        "id": "p6jzxtcmg8Q-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Forest"
      ],
      "metadata": {
        "id": "rqgnJcbjZOke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Random Forest model, like Logistic Regression, also achieved an accuracy of 94% on the test data, meaning 94% of the cases were predicted correctly.\n",
        "\n",
        "However, again, while the confusion matrix showed that the model correctly identified 960 people without a stroke, without any mistakes; it missed all of the stroke cases and misclassified 62 people who had a stroke as not having one.\n",
        "\n",
        "Revealing the Random Forest model is good at predicting non-stroke cases, it also struggles to identify actual stroke cases.\n"
      ],
      "metadata": {
        "id": "bLPqw1Foqx21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#K-Nearest Neighbours"
      ],
      "metadata": {
        "id": "24JilSc-ZUGX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The K-Nearest Neighbours (KNN) model achieved an accuracy of 93% on the test data, as a result 93% of the cases were predicted correctly.\n",
        "\n",
        "The confusion matrix revealed the model accurately predicted 955 non-stroke cases, without any mistakes. However it misclassified 62 people who had a stroke as not having one. It also incorrectly predicted 5 people as having a stroke when they did not have a stroke. None of the stroke cases were correctly identified.\n",
        "\n",
        "This reveals the KNN model is fairly accurate at predicting non stroke cases but struggles to identify the minority class of stroke cases."
      ],
      "metadata": {
        "id": "Axk2znUcrsZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Decision Tree#"
      ],
      "metadata": {
        "id": "Uc3klenH13Hx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Decision Tree model achieved an accuracy of 91% on the test data, meaning 91% of the cases were predicted correctly.\n",
        "\n",
        "The confusion matrix showed that the model correctly identified 913 people without a stroke but misclassified 47 people as having a stroke when they did not. Unlike the other models, the Decision Tree model correctly identified 14 actual stroke cases but still misclassified 48 stroke cases as not having had a stroke.\n",
        "\n",
        "Although the accuracy for the Decision Tree model is lower than the other models at 91%, it is still the only model to accurately predict any stroke cases. Therefore, the Decision Tree model is currently the best model for predicting stroke cases, even if its accuracy for non-stroke cases is lower. However, the Decision Tree model still misclassified 48 stroke cases, therefore like the other models, it still struggles to identify the minority class of stroke cases.\n"
      ],
      "metadata": {
        "id": "yagHJnSN19Sx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classification report"
      ],
      "metadata": {
        "id": "87tz3Vgh0Y2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#Make predictions of the test set for logisitic regression\n",
        "y_pred = logistic_regression_model.model.predict(X_test)\n",
        "# Print the classification report\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "#Make predicition of the test set for random forest\n",
        "y_pred = random_forest_model.model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "#Make predictions of the test set for KNN\n",
        "y_pred = knn_model.model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "#Make predictions of the test set for decision tree\n",
        "y_pred = decision_tree_model.model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "Z_nFcCLY0iyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classification report provides an evaluation of the performance of the models. It includes precision, recall, f1-score and support.  \n",
        "\n",
        "#**Logistic Regression and Random Forest:**#\n",
        "\n",
        "**Class 0 (Non-Stroke)**\n",
        "- Precision: 0.94. This result means the models accurately predicted 94% of the non-stroke cases as correct.\n",
        "- Recall: 1.00. This result means 100% of the non-stroke cases were accurately identified by both models.\n",
        "- F1-Score: 0.97. The f1 score balances precision and recall. a score of 0.97 is high and thus, the models are good at predicting non-stroke cases.\n",
        "- Support: 960. This is the actual number of non-stroke cases within the test data.\n",
        "\n",
        "**Class 1 (Stroke)**\n",
        "- Precision: 0.00. This result means the models did not predict any stroke cases, as a result there were 0 stroke predictions.\n",
        "- Recall: 0.00. This result means 0% of the stroke cases were identified by the model.\n",
        "- F1-Score: 0.00. The precision and recall scores are 0 and as a result the f1 score is also 0.\n",
        "- Support: This is the actual number of stroke cases within the test data.\n",
        "\n",
        "\n",
        "**Accuracy**\n",
        "- The overall accuracy of the model is 0.94 which means it accurately predicted 94% of the total cases which includes stroke and non-stroke.\n",
        "\n",
        "\n",
        "**Macro Average**\n",
        "- Precision: 0.88, Recall: 0.94, F1-Score: 0.48\n",
        "The Macro average is the unweighted average of precision, recall and the f1-score. The model works well when predicted non-stroke cases (class 0) but poorly when predicting stroke cases (class 1) and as a result, the macro average is lower compared to the weighted averages.\n",
        "\n",
        "\n",
        "**Weighted Average**\n",
        "- Precision: 0.88, Recall: 0.94, F1-Score: 0.91\n",
        "The weighted average takes into account the number of cases there are in each class, stroke and non-stroke. The dataset consists of significantly more non-stroke cases than stroke cases, thus increasing the overall scores making them higher.\n",
        "\n",
        "**Warning**\n",
        "- When the code is run, it results in a warning message at the bottom. As the model did not predict any stroke cases, the precision for class 1 is not defined. As a result, the precision is set to 0 which leads to a warning about dividing by zero.  \n",
        "\n",
        "#**K-Nearest Neighbours (KNN):**#\n",
        "\n",
        "**Class 0 (Non-Stroke)**\n",
        "- Precision: 0.94. This result means the model accurately predicted 94% of the non-stroke cases as correct.\n",
        "- Recall: 0.99. This result means 99% of the non-stroke cases were accurately identified by the model.\n",
        "- F1-Score: 0.97. The F1 score balances precision and recall. A score of 0.97 is high, which means the model is good at predicting non-stroke cases.\n",
        "Support: 960. This is the number of non-stroke cases within the test data.\n",
        "\n",
        "**Class 1 (Stroke)**\n",
        "- Precision: 0.00. This result means the model did not predict any stroke cases, as a result, there were 0 stroke predictions.\n",
        "Recall: 0.00. This result means 0% of the stroke cases were identified by the model.\n",
        "- F1-Score: 0.00. The precision and recall scores are 0, as a result, the F1 score is also 0.\n",
        "- Support: 62. This is the actual number of stroke cases within the test data.\n",
        "Accuracy\n",
        "\n",
        "The overall accuracy of the KNN model is 0.93, which means it accurately predicted 93% of the total cases, which includes both stroke and non-stroke.\n",
        "\n",
        "**Macro Average**\n",
        "\n",
        "- Precision: 0.47, Recall: 0.50, F1-Score: 0.48.\n",
        "The macro average is the unweighted average of precision, recall, and the - F1-score. The model works well when predicting non-stroke cases (Class 0) but poorly when predicting stroke cases (Class 1), and as a result, the macro average is lower compared to the weighted averages.\n",
        "\n",
        "**Weighted Average**\n",
        "\n",
        "- Precision: 0.88, Recall: 0.93, F1-Score: 0.91.\n",
        "The weighted average takes into account the number of cases in each class, stroke and non-stroke. The dataset consists of significantly more non-stroke cases than stroke cases, thus increasing the overall scores and making them higher.\n",
        "\n",
        "**Warning**\n",
        "\n",
        "When the code is run, it results in a warning message at the bottom. As the model did not predict any stroke cases, the precision for Class 1 is not defined. As a result, the precision is set to 0, which leads to a warning about dividing by zero.\n",
        "\n",
        "#**Decision Tree:**#\n",
        "\n",
        "**Class 0 (Non-Stroke)**\n",
        "\n",
        "- Precision: 0.95. This result means the model accurately predicted 95% of the non-stroke cases as correct.\n",
        "- Recall: 0.95. This result means 95% of the non-stroke cases were accurately identified by the model.\n",
        "- F1-Score: 0.95. The F1 score balances precision and recall. A score of 0.95 is relatively high, meaning the model is effective at predicting non-stroke cases.\n",
        "- Support: 960. This is the number of non-stroke cases within the test data.\n",
        "\n",
        "**Class 1 (Stroke)**\n",
        "\n",
        "- Precision: 0.23. This result means the model accurately predicted 23% of the stroke cases it classified as strokes.\n",
        "Recall: 0.23. This result means 23% of the actual stroke cases were correctly identified by the model.\n",
        "- F1-Score: 0.23. The F1 score balances precision and recall. A score of 0.23 shows that the model is still struggling to accurately predict stroke cases.\n",
        "- Support: 62. This is the actual number of stroke cases within the test data.\n",
        "\n",
        "**Accuracy**\n",
        "\n",
        "The overall accuracy of the Decision Tree model is 0.91, meaning it accurately predicted 91% of the total cases, including both stroke and non-stroke.\n",
        "\n",
        "**Macro Average**\n",
        "\n",
        "- Precision: 0.59, Recall: 0.59, F1-Score: 0.59.\n",
        "The macro average is the unweighted average of precision, recall, and the F1-score. The Decision Tree model has a more balanced performance across both classes compared to the other models, but it is still not very effective at predicting stroke cases.\n",
        "\n",
        "**Weighted Average**\n",
        "\n",
        "- Precision: 0.91, Recall: 0.91, F1-Score: 0.91.\n",
        "The weighted average takes into account the number of cases in each class, stroke and non-stroke. Since there are significantly more non-stroke cases than stroke cases, the overall scores are higher, reflecting the model’s strength in predicting the majority class.\n",
        "\n"
      ],
      "metadata": {
        "id": "dQUePuYD035M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# Using OOP to create a 'model' class.\n",
        "class Model:\n",
        "    def __init__(self, model, name):\n",
        "        self.model = model\n",
        "        self.name = name\n",
        "\n",
        "#Training the data with the x and y variables\n",
        "    def train(self, X_train, y_train):\n",
        "        self.model.fit(X_train, y_train)\n",
        "\n",
        "#Evaluating the model performance using 20% of the dataset that was previously split. This will help assess how well the models makes predictions using only the test data.\n",
        "#y_pred is a variable that holds the predictions made by the model.\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        y_pred = self.model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "        return accuracy, conf_matrix\n",
        "\n",
        "#Displaying the results.\n",
        "    def display_results(self, accuracy, conf_matrix):\n",
        "        print(f\"Model: {self.name}\")\n",
        "        print(f\"Accuracy: {accuracy:.2f}\")\n",
        "        print(f\"Confusion Matrix:\\n{conf_matrix}\\n\")\n",
        "\n",
        "\n",
        "def main():\n",
        "  \"\"\"#Data Import\"\"\"\n",
        "  #Importing data\n",
        "\n",
        "  #Specify the path to your CSV file\n",
        "  file_path = 'https://drive.google.com/uc?export=download&id=1IKUzSjd0qkuLdyT2iC5O8zbpXb-mK_fT'\n",
        "\n",
        "  # Load the CSV file into a pandas DataFrame\n",
        "  df = pd.read_csv(file_path)\n",
        "\n",
        "  # Display the first few rows of the DataFrame\n",
        "  print(df.head())\n",
        "\n",
        "  \"\"\"#Cleaning and Preprocessing the Dataset\"\"\"\n",
        "  df = pd.read_csv(file_path)\n",
        "\n",
        "  # Some values within the bmi section of this dataset are missing therefore I am replacing the missing values by calculating the mean of the non-missing values.\n",
        "  df['bmi'].fillna(df['bmi'].mean(), inplace=True)\n",
        "\n",
        "  # Categorical variables cannot be used directly in machine learning models. One-hot encoding converts categorical variables into numerical columns with binary values (0 or 1). This is necessary for machine learning models to process categorical data. We set drop_first=True to avoid unnecessary columns.\n",
        "  df = pd.get_dummies(df, columns=['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status'], drop_first=True)\n",
        "\n",
        "  # Scale numerical features ensures all numerical data is on a similar scale to ensure no one feature overpowers others.\n",
        "  scaler = StandardScaler()\n",
        "  df[['age', 'avg_glucose_level', 'bmi']] = scaler.fit_transform(df[['age', 'avg_glucose_level', 'bmi']])\n",
        "\n",
        "\n",
        "  \"\"\"#Splitting the Data into Features (x) and Target (y)\"\"\"\n",
        "  # Split data into features (X) and target variable (y). Dropping columns id and stroke as these are irrelevant when training the models to predict if an indidivual has or hasn't had a stroke.\n",
        "  X = df.drop(columns=['id', 'stroke'])\n",
        "  y = df['stroke']\n",
        "\n",
        "  \"\"\"#Splitting Data into Training and Test Sets\"\"\"\n",
        "  #80% of the data will be used to train the model and 20% of it to test the model.\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "  \"\"\"#Training and Evaluating the Models using Test Data with Object Oriented Programming\"\"\"\n",
        "  \"\"\"#Initialising the models\"\"\"\n",
        "  # Initialise models, setting parameters and preparing the models for training and evaluation.\n",
        "  # By using OOP to create a model class, logistic regression, random forest and KNN models are initliased as instances of this class. Thus, allowing for an organised and consistant way to train and evaluate each model.\n",
        "  logistic_regression_model = Model(LogisticRegression(random_state=42), \"Logistic Regression\")\n",
        "  random_forest_model = Model(RandomForestClassifier(random_state=42), \"Random Forest\")\n",
        "  knn_model = Model(KNeighborsClassifier(), \"K-Nearest Neighbors\")\n",
        "  decision_tree_model = Model(DecisionTreeClassifier(random_state=42), \"Decision Tree\")\n",
        "\n",
        "  #Logistic Regression\n",
        "  logistic_regression_model.train(X_train, y_train)\n",
        "  accuracy_lr, conf_matrix_lr = logistic_regression_model.evaluate(X_test, y_test)\n",
        "  logistic_regression_model.display_results(accuracy_lr, conf_matrix_lr)\n",
        "\n",
        "  #Random Forest\n",
        "  random_forest_model.train(X_train, y_train)\n",
        "  accuracy_rf, conf_matrix_rf = random_forest_model.evaluate(X_test, y_test)\n",
        "  random_forest_model.display_results(accuracy_rf, conf_matrix_rf)\n",
        "\n",
        "  #KNN\n",
        "  knn_model.train(X_train, y_train)\n",
        "  accuracy_knn, conf_matrix_knn = knn_model.evaluate(X_test, y_test)\n",
        "  knn_model.display_results(accuracy_knn, conf_matrix_knn)\n",
        "\n",
        "  #Decision Tree\n",
        "  decision_tree_model.train(X_train, y_train)\n",
        "  accuracy_dt, conf_matrix_dt = decision_tree_model.evaluate(X_test, y_test)\n",
        "  decision_tree_model.display_results(accuracy_dt, conf_matrix_dt)\n",
        "\n",
        "  print(f\"Logistic Regression Accuracy: {accuracy_lr:.2f}\")\n",
        "  print(f\"Random Forest Accuracy: {accuracy_rf:.2f}\")\n",
        "  print(f\"K-Nearest Neighbors Accuracy: {accuracy_knn:.2f}\")\n",
        "  print(f\"Decision Tree Accuracy: {accuracy_dt:.2f}\")\n",
        "\n",
        "  \"\"\"#Making a table to show the results of each model\"\"\"\n",
        "  #Creating a dictionary to store the model names, accuracies, and parameters (if any)\n",
        "  data = {\n",
        "    'Machine Learning Model': ['Logistic Regression', 'Random Forest', 'K-Nearest Neighbors', 'Decision Tree'],\n",
        "    'Optimal Accuracy': [accuracy_lr, accuracy_rf, accuracy_knn, accuracy_dt],\n",
        "\n",
        "  }\n",
        "\n",
        "  #Creating a pandas DataFrame from the dictionary\n",
        "  df_results = pd.DataFrame(data)\n",
        "\n",
        "  #Display the table\n",
        "  #creating a new line before the df_results\n",
        "  print()\n",
        "  print(df_results)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "aYFjs7JjaqdH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de8bffc0-bc0b-49be-8cab-6e1a5b5dbf51"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      id  gender   age  hypertension  heart_disease ever_married  \\\n",
            "0   9046    Male  67.0             0              1          Yes   \n",
            "1  51676  Female  61.0             0              0          Yes   \n",
            "2  31112    Male  80.0             0              1          Yes   \n",
            "3  60182  Female  49.0             0              0          Yes   \n",
            "4   1665  Female  79.0             1              0          Yes   \n",
            "\n",
            "       work_type Residence_type  avg_glucose_level   bmi   smoking_status  \\\n",
            "0        Private          Urban             228.69  36.6  formerly smoked   \n",
            "1  Self-employed          Rural             202.21   NaN     never smoked   \n",
            "2        Private          Rural             105.92  32.5     never smoked   \n",
            "3        Private          Urban             171.23  34.4           smokes   \n",
            "4  Self-employed          Rural             174.12  24.0     never smoked   \n",
            "\n",
            "   stroke  \n",
            "0       1  \n",
            "1       1  \n",
            "2       1  \n",
            "3       1  \n",
            "4       1  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-cd115c5c577b>:54: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['bmi'].fillna(df['bmi'].mean(), inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: Logistic Regression\n",
            "Accuracy: 0.94\n",
            "Confusion Matrix:\n",
            "[[960   0]\n",
            " [ 62   0]]\n",
            "\n",
            "Model: Random Forest\n",
            "Accuracy: 0.94\n",
            "Confusion Matrix:\n",
            "[[960   0]\n",
            " [ 62   0]]\n",
            "\n",
            "Model: K-Nearest Neighbors\n",
            "Accuracy: 0.93\n",
            "Confusion Matrix:\n",
            "[[955   5]\n",
            " [ 62   0]]\n",
            "\n",
            "Model: Decision Tree\n",
            "Accuracy: 0.91\n",
            "Confusion Matrix:\n",
            "[[913  47]\n",
            " [ 48  14]]\n",
            "\n",
            "Logistic Regression Accuracy: 0.94\n",
            "Random Forest Accuracy: 0.94\n",
            "K-Nearest Neighbors Accuracy: 0.93\n",
            "Decision Tree Accuracy: 0.91\n",
            "\n",
            "  Machine Learning Model  Optimal Accuracy\n",
            "0    Logistic Regression          0.939335\n",
            "1          Random Forest          0.939335\n",
            "2    K-Nearest Neighbors          0.934442\n",
            "3          Decision Tree          0.907045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "WDbq_wo0HJs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective of this project was to create a machine learning model that is able to accurately predict a stroke. I used four models, Logistic Regression, Random Forest, KNN, and Decision Tree to do this. These models were trained on a dataset with multiple health-related features to identify patterns that are likely to indicate a stroke.\n",
        "\n",
        "Although all of the models achieved high accuracy, with Logistic Regression and Random Forest at 94%, the models struggled to identify the minority class of stroke cases. As a result, the models all had low performance for stroke detection.\n",
        "\n",
        "Of all the models, I believe the Decision Tree is currently the best at predicting stroke cases. Although its accuracy is lower for non-stroke cases (95%), the Decision Tree model correctly predicted more actual stroke cases compared to all of the other models, despite also having a lower overall accuracy (91%) for the total number of cases.\n",
        "\n",
        "The dataset used is very unbalanced, with far more non-stroke cases than stroke cases. The model could be improved if the dataset was more balanced. Addressing the imbalnace in data with techniques such as class-weighting (givign the minorty class a higher weight) or SMOTE (creating synthetic examples of the minorty class) and undersampling (reducing the number of majority class)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "If the dataset had a more even number of stroke vs non-stroke cases, the models would be better trained and therefore have better stroke detection performance."
      ],
      "metadata": {
        "id": "IE2zMdOxHRiv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rrk2_grsxD8z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}